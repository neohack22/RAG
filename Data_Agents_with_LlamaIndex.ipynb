{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/RAG/blob/main/Data_Agents_with_LlamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Agents with LLamaIndex\n",
        "\n",
        "In LlamaIndex v0.10, the infrastructure for agents is centered around the idea of a Data Agent, or what they call an “LLM-powered knowledge worker.” While the query engines at the heart of LlamaIndex can read from vector databases, data agents can dynamically deal with data from vector databases and a host of external tools.\n",
        "\n",
        "​In this event, we deep dive into how LlamaIndex does agents, and how agents interact with the core constructs of nodes and query engines. We build a complex RAG capable of answering questions by reasoning through quantitative (structured) and qualitative (unstructured) information."
      ],
      "metadata": {
        "id": "x5TTdyIoo3i7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcvGuykZSV_8"
      },
      "source": [
        "### A note on terminology:\n",
        "\n",
        "You'll notice that there are quite a few similarities between LangChain and LlamaIndex. LlamaIndex can largely be thought of as an extension to LangChain, in some ways - but they moved some of the language around. Let's spend a few moments disambiguating the language.\n",
        "\n",
        "- `QueryEngine` -> `LCEL Chain`:\n",
        "  -  `QueryEngine` is just LlamaIndex's way of indicating something is an LLM \"chain\" on top of a retrieval system\n",
        "- `OpenAIAgent` vs. `Agent`:\n",
        "  - The two agents have the same fundamental pattern: Decide which of a list of tools to use to answer a user's query.\n",
        "  - `OpenAIAgent` (LlamaIndex's primary agent) does not need to rely on an agent excecutor due to the fact that it is leveraging OpenAI's [functional api](https://openai.com/blog/function-calling-and-other-api-updates) which allows the agent to interface \"directly\" with the tools instead of operating through an intermediary application process.\n",
        "\n",
        "There is, however, a much large terminological difference when it comes to discussing data.\n",
        "\n",
        "##### Nodes vs. Documents\n",
        "\n",
        "As you're aware of from the previous weeks assignments, there's an idea of `documents` in NLP which refers to text objects that exist within a corpus of documents.\n",
        "\n",
        "LlamaIndex takes this a step further and reclassifies `documents` as `nodes`. Confusingly, it refers to the `Source Document` as simply `Documents`.\n",
        "\n",
        "The `Document` -> `node` structure is, almost exactly, equivalent to the `Source Document` -> `Document` structure found in LangChain - but the new terminology comes with some clarity about different structure-indices.\n",
        "\n",
        "We won't be leveraging those structured indicies today, but we will be leveraging a \"benefit\" of the `node` structure that exists as a default in LlamaIndex, which is the ability to quickly filter nodes based on their metadata.\n",
        "\n",
        "![image](https://i.imgur.com/B1QDjs5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msvju70DSV_8"
      },
      "source": [
        "## BOILERPLATE\n",
        "\n",
        "This is only relevant when running the code in a Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VF57GcmSV_9"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zQBPs6zSV_9"
      },
      "source": [
        "## Load Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhEbxl3nSV_9"
      },
      "source": [
        "Let's grab our core `llama-index` library, as well as OpenAI's Python SDK.\n",
        "\n",
        "We'll be leveraging OpenAI's suite of APIs to power our RAG pipelines today.\n",
        "\n",
        "> NOTE: You can safely ignore any pip errors that occur during the running of these cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3oyvtl2SV_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406dd7b9-194d-4a51-8162-759dac6c90b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.1/136.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\"\"\"!pip install -qU llama-index openai\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be collecting our semantic data from Wikipedia - and so will need the [Wikipedia Reader](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-wikipedia)!"
      ],
      "metadata": {
        "id": "uqTXRTrLPatl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "!pip install -qU llama-index openai\n",
        "```\n",
        "\n",
        "La commande \"!pip install\" est une commande utilisée dans l'environnement Python pour installer des packages depuis le Python Package Index (PyPI).\n",
        "\n",
        "- \"!\" est utilisé pour exécuter une commande shell à partir de Jupyter Notebook ou de l'interpréteur de commandes.\n",
        "- \"pip\" est le gestionnaire de packages Python.\n",
        "- \"install\" est une sous-commande de pip qui permet d'installer des packages.\n",
        "- \"-q\" ou \"--quiet\" est un drapeau optionnel pour exécuter l'installation en mode silencieux, c'est-à-dire sans afficher de messages de progression.\n",
        "- \"-U\" ou \"--upgrade\" est un drapeau optionnel qui indique à pip de mettre à jour le package s'il est déjà installé.\n",
        "- \"llama-index\" et \"openai\" sont les noms des packages que vous souhaitez installer ou mettre à jour.\n",
        "\n",
        "Ainsi, cette commande installe ou met à jour les packages \"llama-index\" et \"openai\" depuis PyPI de manière silencieuse."
      ],
      "metadata": {
        "id": "IstPbB_iHSDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index openai"
      ],
      "metadata": {
        "id": "k93Wit7LG-Xm",
        "outputId": "86c7c895-8ff6-4eea-e7ae-92b030122136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.7/308.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.1/136.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9kXf7pPSV_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df33ce7-9699-4c71-b002-0533950dc1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\"\"\"!pip install -qU wikipedia llama-index-readers-wikipedia\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "!pip install -qU wikipedia llama-index-readers-wikipedia\n",
        "```\n",
        "\n",
        "Cette commande \"!pip install\" installe ou met à jour les packages \"wikipedia\" et \"llama-index-readers-wikipedia\" depuis PyPI.\n",
        "\n",
        "- \"wikipedia\" est une bibliothèque Python qui facilite l'accès aux données de Wikipédia.\n",
        "- \"llama-index-readers-wikipedia\" est une extension de la bibliothèque \"llama-index\" qui fournit des fonctionnalités spécifiques pour lire les données de Wikipédia à partir de l'index de Llama.\n",
        "\n",
        "L'utilisation de \"-q\" ou \"--quiet\" rend l'installation silencieuse, c'est-à-dire sans afficher de messages de progression. Le drapeau \"-U\" ou \"--upgrade\" permet de mettre à jour les packages s'ils sont déjà installés."
      ],
      "metadata": {
        "id": "rWB92VRLIjYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wikipedia llama-index-readers-wikipedia"
      ],
      "metadata": {
        "id": "zMVKr4h7InNm",
        "outputId": "3d8c861d-ddb2-4476-ea4d-b80147d75f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our vector database today will be powered by [QDrant](https://qdrant.tech/) and so we'll need that package as well!"
      ],
      "metadata": {
        "id": "Qq_rWHZEPrCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "!pip install -qU llama-index-vector-stores-qdrant qdrant-client\n",
        "```\n",
        "\n",
        "Cette commande \"!pip install\" installe ou met à jour les packages \"llama-index-vector-stores-qdrant\" et \"qdrant-client\" depuis PyPI.\n",
        "\n",
        "- \"llama-index-vector-stores-qdrant\" est une extension de la bibliothèque \"llama-index\" qui fournit des fonctionnalités spécifiques pour les magasins de vecteurs utilisant Qdrant.\n",
        "- \"qdrant-client\" est une bibliothèque cliente Python pour interagir avec Qdrant, un moteur de recherche et d'indexation de vecteurs.\n",
        "\n",
        "Les options \"-q\" ou \"--quiet\" rendent l'installation silencieuse, c'est-à-dire sans afficher de messages de progression. Le drapeau \"-U\" ou \"--upgrade\" permet de mettre à jour les packages s'ils sont déjà installés."
      ],
      "metadata": {
        "id": "Xztiwi8PJHi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU llama-index-vector-stores-qdrant qdrant-client"
      ],
      "metadata": {
        "id": "I053WsIYJRNJ",
        "outputId": "88550639-ccf5-48bb-df8c-01ba7d2f0643",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.2/223.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"!pip install -qU llama-index-vector-stores-qdrant qdrant-client\"\"\""
      ],
      "metadata": {
        "id": "iMCA8HFGEPF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll need to grab a few dependencies related to our quantitative data!"
      ],
      "metadata": {
        "id": "wNHDqna1P1NQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrT8a3c0SV__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856dba06-fba1-4102-e5a1-17c16764b238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\"\"\"!pip install -q -U sqlalchemy pandas\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "!pip install -q -U sqlalchemy pandas\n",
        "```\n",
        "\n",
        "Cette commande \"!pip install\" installe ou met à jour les packages \"sqlalchemy\" et \"pandas\" depuis PyPI.\n",
        "\n",
        "- \"sqlalchemy\" est une bibliothèque Python qui facilite l'interaction avec les bases de données relationnelles en utilisant le langage SQL de manière transparente.\n",
        "- \"pandas\" est une bibliothèque Python largement utilisée pour la manipulation et l'analyse de données, offrant des structures de données puissantes et des outils pour travailler avec des données tabulaires et de séries chronologiques.\n",
        "\n",
        "Les options \"-q\" ou \"--quiet\" rendent l'installation silencieuse, c'est-à-dire sans afficher de messages de progression. Le drapeau \"-U\" ou \"--upgrade\" permet de mettre à jour les packages s'ils sont déjà installés."
      ],
      "metadata": {
        "id": "hu1cwSjkJkeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U sqlalchemy pandas"
      ],
      "metadata": {
        "id": "fWhP4KRvJo3D",
        "outputId": "9392ff95-33b5-4b82-9315-57baef7901b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional Dependency"
      ],
      "metadata": {
        "id": "6Jifl-qHO8MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll can use [Weights and Biases](https://docs.wandb.ai/guides/prompts) (WandB) as a visibility platform, as well as storing our index!"
      ],
      "metadata": {
        "id": "SS3I4FUabE-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"!pip install -qU wandb llama-index-callbacks-wandb\"\"\""
      ],
      "metadata": {
        "id": "9UcQpjG9Ovlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57208f57-6196-40c7-e599-b1d3fa9cd594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "!pip install -qU wandb llama-index-callbacks-wandb\n",
        "```\n",
        "\n",
        "Cette commande \"!pip install\" installe ou met à jour les packages \"wandb\" et \"llama-index-callbacks-wandb\" depuis PyPI.\n",
        "\n",
        "- \"wandb\" est une bibliothèque Python qui facilite la surveillance et la visualisation des expériences d'apprentissage automatique. Elle est souvent utilisée pour suivre les métriques d'entraînement, les performances des modèles, et pour la collaboration entre les membres de l'équipe.\n",
        "- \"llama-index-callbacks-wandb\" est une extension de la bibliothèque \"llama-index\" qui fournit des fonctionnalités spécifiques pour intégrer les données d'indexation avec Weights & Biases (W&B), une plateforme de suivi et de collaboration pour l'apprentissage automatique expérimental.\n",
        "\n",
        "Les options \"-q\" ou \"--quiet\" rendent l'installation silencieuse, c'est-à-dire sans afficher de messages de progression. Le drapeau \"-U\" ou \"--upgrade\" permet de mettre à jour les packages s'ils sont déjà installés."
      ],
      "metadata": {
        "id": "A-Ba-D52J968"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wandb llama-index-callbacks-wandb"
      ],
      "metadata": {
        "id": "-CZCiMMpKLeZ",
        "outputId": "34693ac9-0ffd-4062-db88-314069770010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OHSwkDySV_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6298eec3-e5b6-41a4-b276-0c4fb2c4911f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WandB API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "\"\"\"import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"WandB API Key: \")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"WandB API Key: \")\n",
        "```\n",
        "\n",
        "Ce code permet de définir la clé API de Weights & Biases (W&B) en tant que variable d'environnement. L'utilisateur est invité à saisir la clé API à l'aide de la fonction `getpass.getpass()`, qui masque l'entrée de la clé. Une fois saisie, la clé API est stockée dans la variable d'environnement `WANDB_API_KEY`, ce qui permet à W&B d'authentifier l'utilisateur pour l'utilisation de ses services."
      ],
      "metadata": {
        "id": "Xf5n_YfLKX2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"WandB API Key: \")"
      ],
      "metadata": {
        "id": "LKB2gEPiKdd4",
        "outputId": "593595be-99d1-4bd7-ccd9-b7204b91310a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ff69d8bda7de>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"WANDB_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WandB API Key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    832\u001b[0m             warnings.warn(\"The `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\",\n\u001b[1;32m    833\u001b[0m                     UserWarning, stacklevel=2)\n\u001b[0;32m--> 834\u001b[0;31m         return self._input_request(prompt,\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need to set a callback handler for WandB to ensure smooth operation of our traces!"
      ],
      "metadata": {
        "id": "Ut8E447rQBjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoV2JINuSV_-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "707c2d11-e096-440b-fc29-179cc3377803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LlamaIndex events to W&B at https://wandb.ai/chrisalexiuk/data-agents-demo/runs/3r7glqkw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbCallbackHandler` is currently in beta.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `llamaindex`.\n"
          ]
        }
      ],
      "source": [
        "import llama_index\n",
        "from llama_index.core import set_global_handler\n",
        "\n",
        "set_global_handler(\"wandb\", run_args={\"project\": \"data-agents-demo\"})\n",
        "wandb_callback = llama_index.core.global_handler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Set Env Variables and Set Up WandB Callback\n",
        "\n",
        "Let's set our API keys for both OpenAI and WandB!"
      ],
      "metadata": {
        "id": "ibtguieHP6n1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IBgMTPTSV_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef92a3f4-216e-4363-fead-16649a179646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQwigweOSV_-"
      },
      "source": [
        "### Task 3: Settings\n",
        "\n",
        "LlamaIndex lets us set global settings which we can use to influence the default behaviour of our components.\n",
        "\n",
        "Let's set our LLM and our Embedding Model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KOy21KPSV_-"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiOdw7EQSV_-"
      },
      "source": [
        "## `Index` Creation\n",
        "\n",
        "In order for us to perform RAG in the traditional sense - we need an `Index`.\n",
        "\n",
        "So what is an `Index`? Well - let's see how LlamaIndex defines it:\n",
        "\n",
        "> In LlamaIndex terms, an `Index` is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.\n",
        "\n",
        "Okay, so we know that we have a boatload of Wikipedia content - and we know that we want to be able to query the `Index` and receive documents that are related to our query - so let's use an `Index` built on the idea of embedding-vectors.\n",
        "\n",
        "Introducing: `VectorStoreIndex`!\n",
        "\n",
        "Again, let's see how LlamaIndex defines this:\n",
        "\n",
        "> A `VectorStoreIndex` is by far the most frequent type of `Index` you'll encounter. The Vector Store Index takes your Documents and splits them up into Nodes. It then creates `vector` embeddings of the text of every node, ready to be queried by an LLM.\n",
        "\n",
        "Alright, that sounds awesome - let's make one!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDEtQQWnSV_-"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We're just going to be pulling information straight from Wikipedia using the built in `WikipediaReader`.\n",
        "\n",
        "> NOTE: Setting `auto_suggest=False` ensures we run into fewer auto-correct based errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrzzrnYMSV_-"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.wikipedia import WikipediaReader\n",
        "\n",
        "movie_list = [\n",
        "    \"Dune (2021 film)\",\n",
        "    \"Dune: Part Two\",\n",
        "    \"Harry Potter and the Philosopher's Stone (film)\",\n",
        "    \"Harry Potter and the Chamber of Secrets (film)\",\n",
        "    \"The Lord of the Rings: The Fellowship of the Ring\",\n",
        "    \"The Lord of the Rings: The Two Towers\",\n",
        "    \"The Hobbit: An Unexpected Journey\",\n",
        "    \"The Hobbit: The Desolation of Smaug\"\n",
        "]\n",
        "\n",
        "wiki_docs = WikipediaReader().load_data(pages=movie_list, auto_suggest=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6FRQ4hfSV_-"
      },
      "source": [
        "### Initializing our `VectorStoreIndex` with QDrant\n",
        "\n",
        "QDrant is a locally hostable and open-source vector database solution.\n",
        "\n",
        "It offers powerful features like metadata filtering out of the box, and will suit our needs well today!\n",
        "\n",
        "We'll start by creating our local `:memory:` client (in-memory and not meant for production use-cases) and our collection."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"movie_wikis\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")"
      ],
      "metadata": {
        "id": "b1ut96aSEVwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c5b16c-21f7-4459-dccc-46f92bf58250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we'll create our `VectorStore` and `StorageContext` which will allow us to create an empty `VectorStoreIndex` which we will be able to add nodes to later!"
      ],
      "metadata": {
        "id": "Fa3mbrfGnkCk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZ5xilm1SV_-"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"movie_wikis\")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    [],\n",
        "    storage_context=storage_context,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qtzVDelSV_-"
      },
      "source": [
        "### Node Construction\n",
        "\n",
        "Now we will loop through our documents and metadata and construct nodes.\n",
        "\n",
        "We'll make sure to explicitly associate our nodes with their respective movie so we can filter by the movie title in the upcoming cells.\n",
        "\n",
        "You might be thinking to yourself - wait, we never indicated which embedding model this should use - but remember"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP4INgSGSV_-"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.extractors import TitleExtractor\n",
        "\n",
        "pipeline = IngestionPipeline(transformations=[TokenTextSplitter()])\n",
        "\n",
        "for movie, wiki_doc in zip(movie_list, wiki_docs):\n",
        "  nodes = pipeline.run(documents=[wiki_doc])\n",
        "  for node in nodes:\n",
        "      node.metadata = {\"title\" : movie}\n",
        "  index.insert_nodes(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RAG - QueryEngine\n",
        "\n",
        "Now that we're created our `VectorStoreIndex`, powered by a QDrant VectorStore, we can wrap it in a simple `QueryEngine` using the `as_query_engine()` method - which will connect a few things together for us:"
      ],
      "metadata": {
        "id": "kCSHKnb0mIte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_rag = index.as_query_engine()"
      ],
      "metadata": {
        "id": "3ipb8j6dfLcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we test this out - let's see what information we can find out about from our new `QueryEngine`!"
      ],
      "metadata": {
        "id": "zgJZGr5FqAME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in simple_rag.get_prompts().items():\n",
        "  print(v.get_template())\n",
        "  print(\"\\n~~~~~~~~~~~~~~~~~~\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-3v4ntaqFmd",
        "outputId": "5471e5d3-bc95-4718-80c6-f203f32f521d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: {query_str}\n",
            "Answer: \n",
            "\n",
            "~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "The original query is as follows: {query_str}\n",
            "We have provided an existing answer: {existing_answer}\n",
            "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
            "------------\n",
            "{context_msg}\n",
            "------------\n",
            "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
            "Refined Answer: \n",
            "\n",
            "~~~~~~~~~~~~~~~~~~\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it does!"
      ],
      "metadata": {
        "id": "N1YoR45tnv8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = simple_rag.query(\"Who is the evil Wizard in the story?\")"
      ],
      "metadata": {
        "id": "FgpQhKobfQ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "654d4vdfhnVq",
        "outputId": "f82d1408-a164-4566-c79a-0f684a373380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lord Voldemort'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That makes sense!\n",
        "\n",
        "Let's ask a question that's slightly more...ambiguous."
      ],
      "metadata": {
        "id": "D8rRiijvpVoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = simple_rag.query(\"Who are the giant beings that roam across the world?\")"
      ],
      "metadata": {
        "id": "_m0H1tbLiqME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TEh4yjQzizeV",
        "outputId": "72990004-ef47-4d4e-ff4a-58ea68281eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Stone Giants are the giant beings that roam across the world in the context provided.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the source nodes to see which movies we retrieved."
      ],
      "metadata": {
        "id": "xOb6eh8_pnaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([x.metadata[\"title\"] for x in response.source_nodes])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_qyApFDpPny",
        "outputId": "b57316df-5c6f-41a5-a98f-7f16fb285749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dune (2021 film)', 'The Hobbit: An Unexpected Journey']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so in this case - we've gone with \"Stone Giants\" from the Hobbit.\n",
        "\n",
        "But there's also the sandworms from Dune, and the Ents from Lord of the Rings, and it looks like we got documents from Dune as well.\n",
        "\n",
        "Let's see if there's a way we can use the title metadata we added to filter the results we get!"
      ],
      "metadata": {
        "id": "xBEXFmPapZpg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUa5sHMsSV__"
      },
      "source": [
        "## Auto Retriever Functional Tool\n",
        "\n",
        "This tool will leverage OpenAI's functional endpoint to select the correct metadata filter and query the filtered index - only looking at nodes with the desired metadata.\n",
        "\n",
        "A simplified diagram: ![image](https://i.imgur.com/AICDPav.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2HQHY3pSV__"
      },
      "source": [
        "First, we need to create our `VectoreStoreInfo` object which will hold all the relevant metadata we need for each component (in this case title metadata).\n",
        "\n",
        "Notice that you need to include it in a text list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoAYxbdsSV__"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.vector_stores.types import (\n",
        "    VectorStoreInfo,\n",
        "    MetadataInfo,\n",
        "    ExactMatchFilter,\n",
        "    MetadataFilters,\n",
        ")\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "from typing import List, Tuple, Any\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "top_k = 3\n",
        "\n",
        "vector_store_info = VectorStoreInfo(\n",
        "    content_info=\"information about movies\",\n",
        "    metadata_info=[MetadataInfo(\n",
        "        name=\"title\",\n",
        "        type=\"str\",\n",
        "        description='title of the movie, [\"Dune (2021 film)\", \"Harry Potter and the Philosopher\\'s Stone (film)\", \"The Lord of the Rings: The Fellowship of the Ring\"]'\n",
        "        )]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEjK7jcsSV__"
      },
      "source": [
        "Now we'll create our base PyDantic object that we can use to ensure compatability with our application layer. This verifies that the response from the OpenAI endpoint conforms to this schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yixkwF8zSV__"
      },
      "outputs": [],
      "source": [
        "class AutoRetrieveModel(BaseModel):\n",
        "    query: str = Field(..., description=\"natural language query string\")\n",
        "    filter_key_list: List[str] = Field(\n",
        "        ..., description=\"List of metadata filter field names\"\n",
        "    )\n",
        "    filter_value_list: List[str] = Field(\n",
        "        ...,\n",
        "        description=(\n",
        "            \"List of metadata filter field values (corresponding to names specified in filter_key_list)\"\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep6ORS3FSV__"
      },
      "source": [
        "Now we can build our function that we will use to query the functional endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8sPThxlSV__"
      },
      "outputs": [],
      "source": [
        "def auto_retrieve_fn(\n",
        "    query: str, filter_key_list: List[str], filter_value_list: List[str]\n",
        "):\n",
        "    \"\"\"Auto retrieval function.\n",
        "\n",
        "    Performs auto-retrieval from a vector database, and then applies a set of filters.\n",
        "\n",
        "    \"\"\"\n",
        "    query = query or \"Query\"\n",
        "\n",
        "    exact_match_filters = [\n",
        "        ExactMatchFilter(key=k, value=v)\n",
        "        for k, v in zip(filter_key_list, filter_value_list)\n",
        "    ]\n",
        "    retriever = VectorIndexRetriever(\n",
        "        index, filters=MetadataFilters(filters=exact_match_filters), top_k=top_k\n",
        "    )\n",
        "    query_engine = RetrieverQueryEngine.from_args(retriever)\n",
        "\n",
        "    response = query_engine.query(query)\n",
        "    return str(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt4BV6oISV__"
      },
      "source": [
        "Now we need to wrap our system in a tool in order to integrate it into the larger application.\n",
        "\n",
        "Source Code Here:\n",
        "- [`FunctionTool`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/tools/function_tool.py#L21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4lS1NqeSV__"
      },
      "outputs": [],
      "source": [
        "description = f\"\"\"\\\n",
        "Use this tool to look up non-review based information about films.\n",
        "The schema is given below:\n",
        "{vector_store_info.json()}\n",
        "\"\"\"\n",
        "\n",
        "auto_retrieve_tool = FunctionTool.from_defaults(\n",
        "    fn=auto_retrieve_fn,\n",
        "    name=\"semantic-film-info\",\n",
        "    description=description,\n",
        "    fn_schema=AutoRetrieveModel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZblimmXVSV__"
      },
      "source": [
        "All that's left to do is attach the tool to an OpenAIAgent and let it rip!\n",
        "\n",
        "Source Code Here:\n",
        "- [`OpenAIAgent`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/agent/openai_agent.py#L361)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2hafCTxSV__"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "\n",
        "agent = OpenAIAgent.from_tools(\n",
        "    tools=[auto_retrieve_tool],\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBJWDBK5SV__",
        "outputId": "192c1368-f18b-4454-e71d-e2fbe619a7f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Who starred in the 2021 film?\n",
            "=== Calling Function ===\n",
            "Calling function: semantic-film-info with args: {\"query\":\"Dune (2021 film)\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"Dune (2021 film)\"]}\n",
            "Got output: The 2021 film \"Dune\" is an epic science fiction movie directed by Denis Villeneuve, based on the 1965 novel of the same name by Frank Herbert. It follows the story of Paul Atreides and his family, the noble House Atreides, as they navigate a dangerous conflict on the desert planet Arrakis. The film features a star-studded ensemble cast including Timothée Chalamet, Rebecca Ferguson, Oscar Isaac, Josh Brolin, and others. The movie was well-received by both critics and audiences, praised for various aspects such as direction, screenplay, production values, and staying true to the source material. It was a box office success, grossing over $431 million worldwide.\n",
            "========================\n",
            "\n",
            "The 2021 film \"Dune\" stars Timothée Chalamet, Rebecca Ferguson, Oscar Isaac, Josh Brolin, and others.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"Who starred in the 2021 film?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\"Who are those giant guys from Lord of the Rings that roam around the forest?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZzs2PyDuJGX",
        "outputId": "dd139d14-37ae-40e0-d94d-d20e81e22578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Who are those giant guys from Lord of the Rings that roam around the forest?\n",
            "=== Calling Function ===\n",
            "Calling function: semantic-film-info with args: {\"query\":\"The Lord of the Rings: The Fellowship of the Ring\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"The Lord of the Rings: The Fellowship of the Ring\"]}\n",
            "Got output: The Lord of the Rings: The Fellowship of the Ring is a 2001 epic fantasy adventure film directed by Peter Jackson, based on the first volume of the novel The Lord of the Rings by J. R. R. Tolkien. It follows the story of the Dark Lord Sauron seeking the One Ring to return to power, which has ended up in the possession of the young hobbit Frodo Baggins. Frodo embarks on a dangerous journey with eight companions to Mount Doom in Mordor to destroy the Ring. The film was highly acclaimed for its visual effects, performances, direction, screenplay, and faithfulness to the source material, and it grossed over $868 million worldwide during its original theatrical run.\n",
            "========================\n",
            "\n",
            "The giant guys from \"The Lord of the Rings: The Fellowship of the Ring\" that roam around the forest are likely referring to the Ents. Ents are tree-like creatures in J.R.R. Tolkien's Middle-earth legendarium. They are ancient, sentient beings who protect the forests and have a deep connection to nature.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJcJL7VXSV__"
      },
      "source": [
        "## Quantitative RAG Pipeline with NL2SQL Tooling\n",
        "\n",
        "We'll walk through the steps of creating a natural language to SQL system in the following section.\n",
        "\n",
        "> NOTICE: This does not have parsing on the inputs or intermediary calls to ensure that users are using safe SQL queries. Use this with caution in a production environment without adding specific guardrails from either side of the application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsVcM0-4SV__"
      },
      "source": [
        "The next few steps should be largely straightforward, we'll want to:\n",
        "\n",
        "1. Read in our `.csv` files into `pd.DataFrame` objects\n",
        "2. Create an in-memory `sqlite` powered `sqlalchemy` engine\n",
        "3. Cast our `pd.DataFrame` objects to the SQL engine\n",
        "4. Create an `SQLDatabase` object through LlamaIndex\n",
        "5. Use that to create a `QueryEngineTool` that we can interact with through the `NLSQLTableQueryEngine`!\n",
        "\n",
        "If you get stuck, please consult the documentation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/dune1.csv"
      ],
      "metadata": {
        "id": "WwWOygTqlNBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3712a162-909a-4d1c-be85-32de018aac05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-17 16:13:24--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/dune1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 133391 (130K) [text/plain]\n",
            "Saving to: ‘dune1.csv’\n",
            "\n",
            "\rdune1.csv             0%[                    ]       0  --.-KB/s               \rdune1.csv           100%[===================>] 130.26K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-17 16:13:24 (5.72 MB/s) - ‘dune1.csv’ saved [133391/133391]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/hp_ss.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gd_nO_T1eaS",
        "outputId": "78fe9066-6105-4430-a28d-85c43a7654ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-17 16:13:50--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/hp_ss.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 80384 (78K) [text/plain]\n",
            "Saving to: ‘hp_ss.csv’\n",
            "\n",
            "\rhp_ss.csv             0%[                    ]       0  --.-KB/s               \rhp_ss.csv           100%[===================>]  78.50K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-04-17 16:13:50 (5.94 MB/s) - ‘hp_ss.csv’ saved [80384/80384]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/lotr_fotr.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWFWvEdY1inS",
        "outputId": "fdaf9fb3-b005-4286-b2f2-8152dd389dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-17 16:14:09--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/lotr_fotr.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 172855 (169K) [text/plain]\n",
            "Saving to: ‘lotr_fotr.csv’\n",
            "\n",
            "\rlotr_fotr.csv         0%[                    ]       0  --.-KB/s               \rlotr_fotr.csv       100%[===================>] 168.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-17 16:14:09 (6.89 MB/s) - ‘lotr_fotr.csv’ saved [172855/172855]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PUg-ZuTSWAC"
      },
      "source": [
        "#### Read `.csv` Into Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Hd8PM4SWAC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dune1 = pd.read_csv(\"./dune1.csv\")\n",
        "hp_ss = pd.read_csv(\"./hp_ss.csv\")\n",
        "lotr_fotr = pd.read_csv(\"./lotr_fotr.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPTNyqmpSWAC"
      },
      "source": [
        "#### Create SQLAlchemy engine with SQLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lfuPKYBSWAC"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\"sqlite+pysqlite:///:memory:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJiYiSuHSWAC"
      },
      "source": [
        "#### Convert `pd.DataFrame` to SQL tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-96asUHSWAC",
        "outputId": "bcf30777-dd27-4032-8006-4abd6c917b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "274"
            ]
          },
          "metadata": {},
          "execution_count": 356
        }
      ],
      "source": [
        "dune1.to_sql(\n",
        "  \"Dune\",\n",
        "  engine\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hp_ss.to_sql(\n",
        "  \"Harry Potter\",\n",
        "  engine\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1mHOWRd4NjK",
        "outputId": "4f681219-e3eb-4de3-ceaf-d90833ad306a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "125"
            ]
          },
          "metadata": {},
          "execution_count": 357
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lotr_fotr.to_sql(\n",
        "  \"The Lord of the Rings\",\n",
        "  engine\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R6FTIki4Q51",
        "outputId": "1a5f93cc-813d-40b4-fc83-6608de7c6f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 358
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pibA9qT7SWAC"
      },
      "source": [
        "#### Construct a `SQLDatabase` index\n",
        "\n",
        "Source Code Here:\n",
        "- [`SQLDatabase`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/langchain_helpers/sql_wrapper.py#L9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeDYpR1LSWAD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SQLDatabase\n",
        "\n",
        "short_movie_list=[\n",
        "    \"Dune\",\n",
        "    \"Harry Potter\",\n",
        "    \"The Lord of the Rings\"\n",
        "]\n",
        "\n",
        "sql_database = SQLDatabase(\n",
        "    engine=engine,\n",
        "    include_tables=short_movie_list\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7VfZBenSWAD"
      },
      "source": [
        "#### Create the NLSQLTableQueryEngine interface for all added SQL tables\n",
        "\n",
        "Source Code Here:\n",
        "- [`NLSQLTableQueryEngine`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/indices/struct_store/sql_query.py#L75C1-L75C1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQWSdMtrSWAD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.struct_store.sql_query import NLSQLTableQueryEngine\n",
        "\n",
        "sql_query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=sql_database,\n",
        "    tables=short_movie_list,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu8WfwuTSWAD"
      },
      "source": [
        "#### Wrap It All Up in a `QueryEngineTool`\n",
        "\n",
        "You'll want to ensure you have a descriptive...description!\n",
        "\n",
        "This is what will help the LLM decide which table to use when querying!\n",
        "\n",
        "Sorce Code Here:\n",
        "\n",
        "- [`QueryEngineTool`](https://github.com/jerryjliu/llama_index/blob/d24767b0812ac56104497d8f59095eccbe9f2b08/llama_index/tools/query_engine.py#L13)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DESCRIPTION = \"\"\"This tool should be used to convert natural language to SQL queries that query tables that contain review information about:\n",
        "'Dune'\n",
        "'Harry Potter)'\n",
        "'The Lord of the Rings'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4n567cXVVCtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-mmcBbLSWAD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools.query_engine import QueryEngineTool\n",
        "\n",
        "sql_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=sql_query_engine,\n",
        "    name=\"sql-query\",\n",
        "    description=DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feOrlq4XSWAD"
      },
      "outputs": [],
      "source": [
        "agent = OpenAIAgent.from_tools(\n",
        "    tools=[sql_tool],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT4G6stBSWAD",
        "outputId": "6c3c682a-5965-4840-a9ae-90581630190f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What is the average rating of the Harry Potter movie?\n",
            "=== Calling Function ===\n",
            "Calling function: sql-query with args: {\"input\":\"average rating of the Harry Potter movie\"}\n",
            "Got output: The average rating of the Harry Potter movie is approximately 7.66 out of 10.\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What is the average rating of the Harry Potter movie?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhsoxOpkSWAD",
        "outputId": "96ddf32a-c24d-455e-87a1-5d0366e8c9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average rating of the Harry Potter movie is approximately 7.66 out of 10.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\"What movie series has better reviews, Lord of the Rings or Dune?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FiAS6sF7DoJ",
        "outputId": "2676212d-d35e-4f8e-decc-3e4cc0383af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What movie series has better reviews, Lord of the Rings or Dune?\n",
            "=== Calling Function ===\n",
            "Calling function: sql-query with args: {\"input\": \"average rating of the Lord of the Rings movie\"}\n",
            "Got output: The average rating of the Lord of the Rings movie is approximately 9.87 out of 10. This indicates that the movie is highly regarded by viewers.\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: sql-query with args: {\"input\": \"average rating of the Dune movie\"}\n",
            "Got output: The average rating of the Dune movie is approximately 8.34.\n",
            "========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxDA2Gjm7dDZ",
        "outputId": "cea3ee50-79da-4344-cf6e-1d2f5f206b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average rating of the Lord of the Rings movie is approximately 9.87 out of 10, indicating that it is highly regarded by viewers. On the other hand, the average rating of the Dune movie is approximately 8.34. Therefore, the Lord of the Rings movie series has better reviews compared to Dune.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2LOixbcSWAD"
      },
      "source": [
        "### Multi-Tool Agent Using Query Planning Tool\n",
        "\n",
        "We're going to be leveraging the Query Planning Tool today to help our agent \"come up with a plan\" that it can execute to best answer our questions.\n",
        "\n",
        "Let's start by defining the `QueryPlanTool`!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryPlanTool\n",
        "from llama_index.core import get_response_synthesizer\n",
        "\n",
        "response_synthesizer = get_response_synthesizer()\n",
        "query_plan_tool = QueryPlanTool.from_defaults(\n",
        "    query_engine_tools=[auto_retrieve_tool, sql_tool],\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")"
      ],
      "metadata": {
        "id": "R334HFTtCc03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out how this tool looks!"
      ],
      "metadata": {
        "id": "BOECOkhBCjk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_plan_tool.metadata.to_openai_tool()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNHhftioCjFx",
        "outputId": "74e067c2-c4cc-4fbc-8ed8-871cfa76c81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'function',\n",
              " 'function': {'name': 'query_plan_tool',\n",
              "  'description': '        This is a query plan tool that takes in a list of tools and executes a query plan over these tools to answer a query. The query plan is a DAG of query nodes.\\n\\nGiven a list of tool names and the query plan schema, you can choose to generate a query plan to answer a question.\\n\\nThe tool names and descriptions are as follows:\\n\\n\\n\\n        Tool Name: semantic-film-info\\nTool Description: Use this tool to look up non-review based information about films.\\nThe schema is given below:\\n{\"metadata_info\": [{\"name\": \"title\", \"type\": \"str\", \"description\": \"title of the movie, [\\\\\"Dune (2021 film)\\\\\", \\\\\"Harry Potter and the Philosopher\\'s Stone (film)\\\\\", \\\\\"The Lord of the Rings: The Fellowship of the Ring\\\\\"]\"}], \"content_info\": \"information about movies\"}\\n \\n\\nTool Name: sql-query\\nTool Description: This tool should be used to convert natural language to SQL queries that query tables that contain review information about:\\n\\'Dune\\'\\n\\'Harry Potter)\\'\\n\\'The Lord of the Rings\\'\\n \\n        ',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'nodes': {'title': 'Nodes',\n",
              "     'description': 'The original question we are asking.',\n",
              "     'type': 'array',\n",
              "     'items': {'$ref': '#/definitions/QueryNode'}}},\n",
              "   'required': ['nodes'],\n",
              "   'definitions': {'QueryNode': {'title': 'QueryNode',\n",
              "     'description': 'Query node.\\n\\nA query node represents a query (query_str) that must be answered.\\nIt can either be answered by a tool (tool_name), or by a list of child nodes\\n(child_nodes).\\nThe tool_name and child_nodes fields are mutually exclusive.',\n",
              "     'type': 'object',\n",
              "     'properties': {'id': {'title': 'Id',\n",
              "       'description': 'ID of the query node.',\n",
              "       'type': 'integer'},\n",
              "      'query_str': {'title': 'Query Str',\n",
              "       'description': 'Question we are asking. This is the query string that will be executed. ',\n",
              "       'type': 'string'},\n",
              "      'tool_name': {'title': 'Tool Name',\n",
              "       'description': 'Name of the tool to execute the `query_str`.',\n",
              "       'type': 'string'},\n",
              "      'dependencies': {'title': 'Dependencies',\n",
              "       'description': 'List of sub-questions that need to be answered in order to answer the question given by `query_str`.Should be blank if there are no sub-questions to be specified, in which case `tool_name` is specified.',\n",
              "       'type': 'array',\n",
              "       'items': {'type': 'integer'}}},\n",
              "     'required': ['id', 'query_str']}}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 370
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxFHM2l2SWAD"
      },
      "outputs": [],
      "source": [
        "combined_tool_agent = OpenAIAgent.from_tools(\n",
        "    tools=[auto_retrieve_tool, sql_tool],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYVHuBf9SWAD",
        "outputId": "8fcc2767-14b2-45db-e42a-28890e957567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Which movie is about a chamber, and what is the average rating of the movie?\n",
            "=== Calling Function ===\n",
            "Calling function: semantic-film-info with args: {\"query\":\"movie about a chamber\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"Chamber\"]}\n",
            "Got output: Empty Response\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = combined_tool_agent.chat(\"Which movie is about a chamber, and what is the average rating of the movie?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0w0VbomSWAD",
        "outputId": "e4364aee-f47d-495c-8b21-be204b737033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie \"Harry Potter and the Chamber of Secrets\" is about a chamber. The average rating for this movie is 7.54 out of 10 or 7.2 out of 10.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJvN-vtJSWAD",
        "outputId": "ff3f994f-44de-4be1-f2a7-67f7dbbacacb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What worlds do the LoTR, and Dune movies take place in?\n",
            "=== Calling Function ===\n",
            "Calling function: semantic-film-info with args: {\"query\": \"worlds in LoTR movies\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"The Lord of the Rings: The Fellowship of the Ring\", \"The Lord of the Rings: The Two Towers\"]}\n",
            "Got output: Middle-earth\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: semantic-film-info with args: {\"query\": \"worlds in Dune movies\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"Dune (2021 film)\", \"Dune: Part Two\"]}\n",
            "Got output: The Dune movies feature various worlds, including Caladan, Arrakis, Giedi Prime, and Arrakeen.\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = combined_tool_agent.chat(\"What worlds do the LoTR, and Dune movies take place in?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F74wGv6NSWAD",
        "outputId": "15e1573c-4727-442e-b20e-295a5ee64d0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The \"Lord of the Rings\" movies take place in the world of Middle-earth. On the other hand, the \"Dune\" movies feature various worlds, including Caladan, Arrakis, Giedi Prime, and Arrakeen.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = combined_tool_agent.chat(\"Which of the following movie series is considered the 'best': Harry Potter, Dune, Lord of the Rings, or The Hobbit? Based your answer on both reviews, and non-review information.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvat4hmkvn3N",
        "outputId": "3c43ee4e-ab5d-45f2-fc13-17434b64aec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Which of the following movie series is considered the 'best': Harry Potter, Dune, Lord of the Rings, or The Hobbit? Based your answer on both reviews, and non-review information.\n",
            "=== Calling Function ===\n",
            "Calling function: semantic-film-info with args: {\"query\": \"best movie series based on reviews\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"Harry Potter and the Philosopher's Stone (film)\", \"Harry Potter and the Chamber of Secrets (film)\", \"The Lord of the Rings: The Fellowship of the Ring\", \"The Lord of the Rings: The Two Towers\", \"The Hobbit: An Unexpected Journey\", \"The Hobbit: The Desolation of Smaug\", \"Dune (2021 film)\", \"Dune: Part Two\"]}\n",
            "Got output: Harry Potter and the Philosopher's Stone received positive reviews from critics, with many praising its faithful adaptation of the source material, engaging storyline, and impressive visual effects. The film was particularly commended for its magical elements and exciting sequences, earning it high ratings and accolades.\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: semantic-film-info with args: {\"query\": \"best movie series based on non-review information\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"Harry Potter and the Philosopher's Stone (film)\", \"Harry Potter and the Chamber of Secrets (film)\", \"The Lord of the Rings: The Fellowship of the Ring\", \"The Lord of the Rings: The Two Towers\", \"The Hobbit: An Unexpected Journey\", \"The Hobbit: The Desolation of Smaug\", \"Dune (2021 film)\", \"Dune: Part Two\"]}\n",
            "Got output: Harry Potter and the Philosopher's Stone is considered one of the best movie series based on non-review information, as it achieved significant commercial success, broke numerous box office records, received multiple award nominations, and was well-received by audiences worldwide.\n",
            "========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEOpeuAovtZM",
        "outputId": "b4150bba-3ff7-4a4e-93aa-a3972928e8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Among the movie series mentioned, \"Harry Potter\" is considered one of the best movie series. Additionally, \"Dune\" (2021 film) is also considered one of the best movie series, with its first installment receiving critical acclaim, multiple awards, and a successful box office performance. The film's faithful adaptation of the source material, impressive production values, strong direction, and engaging storyline have contributed to its recognition as one of the standout movie series in recent years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3ELqzOvSWAD"
      },
      "outputs": [],
      "source": [
        "wandb_callback.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}